{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"private_outputs":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["<h1> <b> Sentiment Analysis Project </b> </h1>\n","Team members: Mohamed Tarek, Hady Ahmed, Yousef Ahmed, Mohamed Gaber, Mohamed Allam, and Momen Mohamed <br>\n","\n","T.A : Andrew Magdy\n"],"metadata":{"id":"8delUuLOXt-t"}},{"cell_type":"markdown","source":["<h3> Importing Libraries </h3>"],"metadata":{"id":"eV0r_FiHcKtm"}},{"cell_type":"code","source":["!pip install emot\n","!pip install fastapi\n","!pip install streamlit"],"metadata":{"id":"55MaaQXSBLvd"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1M_wpky0XGZG"},"outputs":[],"source":["import seaborn as sns\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn import metrics\n","import nltk\n","import string\n","import emot\n","import gensim\n","from nltk.stem.porter import PorterStemmer\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import wordnet\n","from nltk.corpus import stopwords\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","from textblob import TextBlob\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import accuracy_score\n","import streamlit as st\n","import joblib\n","from sklearn.preprocessing import FunctionTransformer\n","from sklearn.pipeline import Pipeline\n","\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('vader_lexicon')\n","pd.options.display.max_rows = 4000\n","pd.options.display.max_seq_items = 2000\n"]},{"cell_type":"markdown","source":["<h3> Data Preprocessing </h3>\n","\n","---\n","\n","\n","\n","---\n","\n","\n","Mohamed Allam: Removing punctuation and stop words, and Lowercasing text.\n","<br>\n","Mohamed Tarek: Text normalization and Stemming or lemmatization"],"metadata":{"id":"ExutJGzwX8Dj"}},{"cell_type":"code","source":["# importing data\n","data = pd.read_csv(\"/content/sentimentdataset.csv\")"],"metadata":{"id":"Cu2WYNbGcZWr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(data.head())"],"metadata":{"id":"pYFGoDnmbjIN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(data.info())"],"metadata":{"id":"4ILLPc03bs-6","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data['Timestamp'] = pd.to_datetime(data['Timestamp'])"],"metadata":{"id":"2XNE4RyPOmds"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(data.info())"],"metadata":{"id":"EcYwdOGQO73p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data.describe(include=['O'])"],"metadata":{"id":"VPuOBbNdoRXj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data.drop_duplicates(subset=['Text'],inplace=True)"],"metadata":{"id":"tryEXCg9oq9z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data.reset_index(drop=True, inplace=True)"],"metadata":{"id":"vSiMc4KKw_mu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data.describe(include=['O'])"],"metadata":{"id":"J9jShpvHvHah"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data.describe()"],"metadata":{"collapsed":true,"id":"RZ7yHXWtOIBf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(data[\"Country\"].unique())"],"metadata":{"id":"PuI5PIse-7r_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data['Country'] = data['Country'].str.strip()"],"metadata":{"id":"6YGaDzy8_DQQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(data[\"Country\"].unique())"],"metadata":{"id":"-wANkgvi_QVL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(data[\"Sentiment (Label)\"].unique())"],"metadata":{"id":"8zK4i5qWlEvG","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data[\"Sentiment (Label)\"] = data[\"Sentiment (Label)\"].str.strip()"],"metadata":{"id":"RPkO6v2RlKsd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(data[\"Sentiment (Label)\"].unique())"],"metadata":{"id":"TVTbkRJzqryd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data[\"Sentiment (Label)\"] = data[\"Sentiment (Label)\"].str.lower()"],"metadata":{"id":"_NPrAgiqqIlP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(data[\"Sentiment (Label)\"].unique())"],"metadata":{"id":"P1cLYTrjqsh2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# label = data[\"Sentiment (Label)\"]\n","# print(label.nunique())\n","# label = label.apply(change_to_orgin_stemmer)\n","# print(np.sort(label.unique()))\n","# print(label.nunique())"],"metadata":{"id":"Bv70IZzGq64L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(np.sort(data[\"Sentiment (Label)\"].unique()))"],"metadata":{"id":"60K9CrALexmS","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["positive = ['acceptance', 'accomplishment', 'admiration', 'adoration', 'adrenaline',\n","            'adventure', 'affection', 'amazement', 'amusement', 'anticipation',\n","            'arousal','artisticBurst', 'awe', 'bittersweet', 'blessed',\n","            'breakthrough', 'calmness', 'captivation', 'joy', 'excitement',\n","            'contentment', 'serenity', 'happy', 'nostalgia','hopeful',\n","            'euphoria', 'elation', 'enthusiasm', 'pride',\n","            'determination', 'playful', 'surprise', 'inspiration', 'positive']\n","\n","neutral = ['ambivalence', 'apprehensive', 'boredom', 'curiosity',\n","           'confusion', 'indifference', 'neutral']\n","\n","negative = ['anger', 'anxiety', 'bad', 'betrayal', 'bitter', 'bitterness', 'despair',\n","            'grief', 'sad', 'loneliness', 'embarrassed', 'regret', 'frustration',\n","            'melancholy', 'numbness', 'hate', 'negative']"],"metadata":{"id":"HDs1WucRZPLK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(data[\"Sentiment (Label)\"].value_counts());"],"metadata":{"id":"1o49brR_c8Wl","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def word_polarity(text):\n","    polarity = TextBlob(text)\n","    if text in neutral:\n","      return \"Neutral\"\n","    elif text in positive or polarity.sentiment.polarity >= 0.05 :\n","        return \"Positive\"\n","    elif text in negative or polarity.sentiment.polarity <= -0.05:\n","        return \"Negative\"\n","    else:\n","        return \"Neutral\""],"metadata":{"id":"1JjhAWG5POCj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def word_polarity2(text):\n","    sid = SentimentIntensityAnalyzer()\n","    polarity = sid.polarity_scores(text)['compound']\n","    if text in neutral:\n","        return \"Neutral\"\n","    elif text in positive or polarity >= 0.05:\n","        return \"Positive\"\n","    elif text in negative or polarity <= -0.05:\n","        return \"Negative\"\n","    else:\n","        return \"Neutral\""],"metadata":{"id":"xzj3x8KLSKFe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output = data[\"Sentiment (Label)\"].apply(word_polarity)\n","print(output.value_counts())"],"metadata":{"id":"HxaEZBQDPexZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output = data[\"Sentiment (Label)\"].apply(word_polarity2)\n","data[\"Sentiment (Label)\"] = output\n","print(output.value_counts())"],"metadata":{"id":"BhVdJb4ZSlm6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output.unique()"],"metadata":{"id":"t6D1Kz7W7nQ6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lbl = LabelEncoder()\n","lbl.fit(list(output))\n","output = lbl.transform(list(output))\n","output = pd.DataFrame(output)\n","print(output)\n","# lbl = LabelEncoder()\n","# lbl.fit(list(data[\"Sentiment (Label)\"]))\n","# data[\"Sentiment (Label)\"] = lbl.transform(list(data[\"Sentiment (Label)\"]))\n","# output = pd.DataFrame(output)\n","# print(output)"],"metadata":{"id":"9QeJim1oz54m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["reviews = data[\"Text\"]"],"metadata":{"id":"mJoKMwQeu_k3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["l = pd.concat([reviews, output], axis=1)\n","print(l)"],"metadata":{"id":"2iSnyGsejv-J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h4> Remove Additional Spaces </h4>"],"metadata":{"id":"4mvSwBqR3eul"}},{"cell_type":"code","source":["reviews = reviews.apply(str.strip)"],"metadata":{"id":"YSrSUYUh3eOj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h4> Lowering the Case of Letters </h4>"],"metadata":{"id":"FL1a162t5dBR"}},{"cell_type":"code","source":["reviews = reviews.apply(str.lower)"],"metadata":{"id":"u6tk7XPJ6-4z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h4> Removing Hashtags </h4>"],"metadata":{"id":"y0zWr0Pn47K3"}},{"cell_type":"code","source":["def remove_hashtags(text):\n","  text = text.split()\n","  text = [word for word in text if \"#\" not in word]\n","  # to return it to one string\n","  text = ' '.join(text)\n","  return text"],"metadata":{"id":"BwkYXJVZ2Uee"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["reviews = reviews.apply(remove_hashtags)"],"metadata":{"id":"NFSAfmhK2mlI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h4> Remove Punctuation </h4>"],"metadata":{"id":"pSW3KWCw5JlS"}},{"cell_type":"code","source":["def remove_punctuation(text):\n","  punctuation = string.punctuation\n","  # second argument in maketrans specify what to do with the word that you want to replace\n","  # third argument in maketrans specify what you want to remove\n","  punctuation = punctuation.translate(str.maketrans(\"\",\"\",\"'\"))\n","  text = text.translate(str.maketrans(\"\",\"\",punctuation))\n","  return text"],"metadata":{"id":"3bT2e63M1Fto"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["reviews = reviews.apply(remove_punctuation)"],"metadata":{"id":"GPo9wz0_2Npu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h4> Remove Stop Words </h4>"],"metadata":{"id":"LlgW3vn95CLW"}},{"cell_type":"code","source":["def remove_stop_words(text):\n","  stop_words = set(stopwords.words('english'))\n","  text = text.split()\n","  text = [word for word in text if word not in stop_words]\n","  text = ' '.join(text)\n","  return text"],"metadata":{"id":"UrwgJ8H1uEHI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["stop_words = set(stopwords.words('english'))\n","print(\"not\" in stop_words)"],"metadata":{"id":"LdLALi1pxRdO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["reviews = reviews.apply(remove_stop_words)"],"metadata":{"id":"zW59Oq-qwplC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h4> Remove dash </h4>"],"metadata":{"id":"ExSoi32NapqR"}},{"cell_type":"code","source":["def remove_dash(text):\n","  text = text.translate(str.maketrans(\"\",\"\",\"'\"))\n","  return text"],"metadata":{"id":"lZeYU8urbDyE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["reviews = reviews.apply(remove_dash)"],"metadata":{"id":"JiWbnxCm4zWx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["remove_dash(\"i'am\")"],"metadata":{"id":"W1HchMqKbUzG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h4> Replacing Emojies with Text </h4>"],"metadata":{"id":"rVvaRqUG5oV0"}},{"cell_type":"code","source":["def replace_emojies(text):\n","  emojies_to_words = {value : key.replace(\":\",\"\") for key,value in emot.EMOJI_UNICODE.items()}\n","  emojies_to_words['❤️'] = \"heavy_black_heart\"\n","  text = text.split()\n","  text = [emojies_to_words[word] if word in emojies_to_words.keys() else word for word in text ]\n","  text = ' '.join(text)\n","  return text"],"metadata":{"id":"Hl9tjXSc8zKL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["reviews = reviews.apply(replace_emojies)"],"metadata":{"id":"9N3SB2oqOZww"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h4> Replace Words With their Origin using Stemmer</h4>"],"metadata":{"id":"DkYtJLlG5x8u"}},{"cell_type":"code","source":["def change_to_orgin_stemmer(text):\n","  porter = PorterStemmer()\n","  text = text.split()\n","  text = [porter.stem(word) for word in text]\n","  text = ' '.join(text)\n","  return text"],"metadata":{"id":"xjqi6_PcP15A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["reviews_stemmer = reviews.apply(change_to_orgin_stemmer)"],"metadata":{"id":"qnR2QKr0Qu6x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h4> Replace Words With their Origin using Lemmatizer</h4>"],"metadata":{"id":"cw7F4zQn6DxT"}},{"cell_type":"code","source":["def change_to_orgin_lemmatizer(text):\n","  lemmatizer = WordNetLemmatizer()\n","  wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n","  text = text.split()\n","  text = nltk.pos_tag(text)\n","  text = [lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in text]\n","  text = ' '.join(text)\n","  return text"],"metadata":{"id":"vYKZqPd3RLwe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["reviews_lemmatizer = reviews.apply(change_to_orgin_lemmatizer)"],"metadata":{"id":"vNvgloe5Rbfb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h4> Observing the changes </h4>"],"metadata":{"id":"H7_zFQ8l6Ip6"}},{"cell_type":"code","source":["print(reviews)"],"metadata":{"id":"NoeDXOz9Oh7x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(reviews_stemmer)"],"metadata":{"id":"7JVTnZOsQ7Lb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(reviews_lemmatizer)"],"metadata":{"id":"3eLaBhWHRqlO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(replace_emojies(\"game is on 🥇\"))"],"metadata":{"id":"-uJfSvl92g-3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(lbl.transform([lbl.classes_[0]])[0])"],"metadata":{"id":"e462NhcN20j-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h3>Data Visualizations </h3>"],"metadata":{"id":"hlmZkElf7Fjq"}},{"cell_type":"code","source":["data['Text'] = reviews_lemmatizer"],"metadata":{"id":"aZtmZX5S7Eq9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,8))\n","\n","sns.countplot(data=data, y='Country', hue='Sentiment (Label)', palette=\"viridis_r\", order=data['Country'].value_counts().iloc[:20].index)\n","plt.title(\"Count of The Positive, Negative, and Neutral in Each Country\")"],"metadata":{"id":"gliRdKZe8Gnh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.dates as mdates\n","\n","fig, ax = plt.subplots(1, 1)\n","\n","fig.set_size_inches(10, 8)\n","\n","positive_USA = data[(data['Sentiment (Label)'] == 'Positive')]\n","positive_USA = positive_USA.groupby(data['Timestamp'].dt.year).count()\n","\n","x = positive_USA.index.to_list()\n","x.append(2014)\n","x.sort()\n","plt.xlim(positive_USA.index.min(), positive_USA.index.max() + 1)\n","\n","plt.xticks(x)\n","\n","sns.lineplot(ax=ax, data=positive_USA, x=positive_USA.index, y='ID', color='green')\n","sns.scatterplot(ax=ax, data=positive_USA, x=positive_USA.index, y='ID', color='green')\n","\n","positive_USA = data[(data['Sentiment (Label)'] == 'Negative')]\n","positive_USA = positive_USA.groupby(data['Timestamp'].dt.year).count()\n","\n","sns.lineplot(ax=ax, data=positive_USA, x=positive_USA.index, y='ID', color='red')\n","sns.scatterplot(ax=ax, data=positive_USA, x=positive_USA.index, y='ID', color='red')\n","\n","positive_USA = data[(data['Sentiment (Label)'] == 'Neutral')]\n","positive_USA = positive_USA.groupby(data['Timestamp'].dt.year).count()\n","\n","sns.lineplot(ax=ax, data=positive_USA, x=positive_USA.index, y='ID', color='blue')\n","sns.scatterplot(ax=ax, data=positive_USA, x=positive_USA.index, y='ID', color='blue')\n","\n","ax.legend(handles=ax.lines, labels=[\"Positive\", \"Negative\", \"Neutral\"])\n","\n","plt.tick_params(axis = 'x', labelrotation = 45)\n","\n","plt.title(\"Sentiment Across The Years\")\n","plt.ylabel(\"Count\")\n","plt.xlabel(\"Years\")"],"metadata":{"id":"6vBToqWDA4ea"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["words = {}\n","for i in data['Text']:\n","  word = i.split()\n","  for j in word:\n","    if j in words.keys():\n","      words[j] = words[j] + 1\n","    else:\n","      words[j] = 1\n","sns.barplot(x=words.keys(), y=words.values(), order=sorted(words.keys()))\n"],"metadata":{"id":"vGPvtOauSnWw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h3> Feature Engineering </h3>\n","Momen Mohamed: Word Embeddings\n","<br>\n","Mohamed Gaber: Bag-of-Word\n","<br>\n","Hady Ahmed: TF-IDF"],"metadata":{"id":"d9m7gu1gd3sZ"}},{"cell_type":"markdown","source":["<h4> Checking How Many Words in the Dataset </h4>"],"metadata":{"id":"oMSADUpIS-6J"}},{"cell_type":"code","source":["tokenized_reviews_stemmer = reviews_stemmer.apply(lambda x: x.split())\n","print(tokenized_reviews_stemmer.shape)\n","x=0\n","for i in tokenized_reviews_stemmer :\n","    x=x+len(i)\n","print(x)"],"metadata":{"id":"0bcSfra2LcTR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenized_reviews_lemmatizer = reviews_lemmatizer.apply(lambda x: x.split())\n","print(tokenized_reviews_lemmatizer.shape)\n","x=0\n","for i in tokenized_reviews_lemmatizer :\n","    x=x+len(i)\n","print(x)"],"metadata":{"id":"jJPZuMYIb-uZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h4> Word Embedding using word2vec </h4>"],"metadata":{"id":"xIHlinj73px3"}},{"cell_type":"code","source":["vector_size = 100"],"metadata":{"id":"8XCZzy-WTJWp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenized_reviews_lemmatizer = reviews_lemmatizer.apply(lambda x: x.split()) # tokenizing\n","\n","model_w2v_lemmatizer = gensim.models.Word2Vec(\n","            tokenized_reviews_lemmatizer,\n","            vector_size=vector_size, # desired no. of features/independent variables , important\n","            window=12, # context window size ,number of words consider the meaning of the word\n","            min_count=2,# minimum number of word repiutation to be used in training\n","            sg = 1, # 1 for skip-gram model , to choose the training model\n","            hs = 0,\n","            negative = 10, # for negative sampling\n","            workers= 2, # no.of cpu cores to train the model\n","            seed = 34)\n","\n","model_w2v_lemmatizer.train(tokenized_reviews_lemmatizer, total_examples= len(reviews_lemmatizer), epochs=20)"],"metadata":{"id":"T_8hL1PYeARK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenized_reviews_stemmer = reviews_stemmer.apply(lambda x: x.split()) # tokenizing\n","\n","model_w2v_stemmer = gensim.models.Word2Vec(\n","            tokenized_reviews_stemmer,\n","            vector_size=vector_size, # desired no. of features/independent variables , important\n","            window=12, # context window size ,number of words consider the meaning of the word\n","            min_count=2,# minimum number of word repiutation to be used in training\n","            sg = 1, # 1 for skip-gram model , to choose the training model\n","            hs = 0,\n","            negative = 10, # for negative sampling\n","            workers= 2, # no.of cpu cores to train the model\n","            seed = 34)\n","\n","model_w2v_stemmer.train(tokenized_reviews_stemmer, total_examples= len(reviews_stemmer), epochs=20)"],"metadata":{"id":"jE4ldJMPcKQ3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def word_vector_stemmer(tokens, size):\n","    vec = np.zeros(size).reshape((1, size))\n","    count = 0.\n","    for word in tokens:\n","        try:\n","            vec += model_w2v_stemmer.wv.get_vector(word).reshape((1, size))\n","            count += 1.\n","        except KeyError: # handling the case where the token is not in vocabulary\n","\n","            continue\n","    if count != 0:\n","        vec /= count\n","    return vec"],"metadata":{"id":"ZwS6lhsMTTxP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def word_vector_lemmatizer(tokens, size):\n","    vec = np.zeros(size).reshape((1, size))\n","    count = 0.\n","    for word in tokens:\n","        try:\n","            vec += model_w2v_lemmatizer.wv.get_vector(word).reshape((1, size))\n","            count += 1.\n","        except KeyError: # handling the case where the token is not in vocabulary\n","\n","            continue\n","    if count != 0:\n","        vec /= count\n","    return vec"],"metadata":{"id":"tIhAsldudYkg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["wordvec_arrays = np.zeros((len(tokenized_reviews_stemmer), vector_size))\n","\n","for i in range(len(tokenized_reviews_stemmer)):\n","    wordvec_arrays[i,:] = word_vector_stemmer(tokenized_reviews_stemmer[i], vector_size)\n","\n","reviews_stemmer_word_embedding = pd.DataFrame(wordvec_arrays)"],"metadata":{"id":"QODUVWxhX_kR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["wordvec_arrays = np.zeros((len(tokenized_reviews_lemmatizer), vector_size))\n","\n","for i in range(len(tokenized_reviews_lemmatizer)):\n","    wordvec_arrays[i,:] = word_vector_lemmatizer(tokenized_reviews_lemmatizer[i], vector_size)\n","\n","reviews_lemmatize_word_embedding = pd.DataFrame(wordvec_arrays)\n","print(reviews_lemmatize_word_embedding.shape)"],"metadata":{"id":"i5hMcP2fcQ6H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h4>Bag of Words</h4>"],"metadata":{"id":"z0hIPYYk4Ryo"}},{"cell_type":"code","source":["max_features = 2000\n","max_df = 0.7\n","min_df = 2"],"metadata":{"id":"64W2-t5xRpHb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","bow_vectorizer_stemmer = CountVectorizer(max_df=max_df, min_df=min_df, max_features=max_features, stop_words='english')\n","reviews_stemmer_bag_of_words = bow_vectorizer_stemmer.fit_transform(reviews_stemmer)\n","\n","reviews_stemmer_bag_of_words = pd.DataFrame.sparse.from_spmatrix(reviews_stemmer_bag_of_words)\n","reviews_stemmer_bag_of_words = reviews_stemmer_bag_of_words.sparse.to_dense()"],"metadata":{"id":"AS6j--s-jJOF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","bow_vectorizer_lemmatizer = CountVectorizer(max_df=max_df, min_df=min_df, max_features=max_features, stop_words='english')\n","reviews_lemmatizer_bag_of_words = bow_vectorizer_lemmatizer.fit_transform(reviews_lemmatizer)\n","print(reviews_lemmatizer_bag_of_words.shape)\n","print(type(reviews_lemmatizer_bag_of_words))\n","reviews_lemmatizer_bag_of_words = pd.DataFrame.sparse.from_spmatrix(reviews_lemmatizer_bag_of_words)\n","reviews_lemmatizer_bag_of_words = reviews_lemmatizer_bag_of_words.sparse.to_dense()"],"metadata":{"id":"TQ7hn4jTjlEW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h4>TF-IDF</h4>"],"metadata":{"id":"WNLGd6Ni4iE0"}},{"cell_type":"code","source":["#TF-IDF features\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","tfidf_vectorizer_stemmer = TfidfVectorizer(max_df=max_df, min_df=min_df, max_features=max_features, stop_words='english')\n","reviews_stemmer_tfidf = tfidf_vectorizer_stemmer.fit_transform(reviews_stemmer)\n","\n","reviews_stemmer_tfidf = pd.DataFrame.sparse.from_spmatrix(reviews_stemmer_tfidf)\n","reviews_stemmer_tfidf = reviews_stemmer_tfidf.sparse.to_dense()"],"metadata":{"id":"mgZbXzL9oVdE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#TF-IDF features\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","tfidf_vectorizer_lemmatizer = TfidfVectorizer(max_df=max_df, min_df=min_df, max_features=max_features, stop_words='english')\n","reviews_lemmatizer_tfidf = tfidf_vectorizer_lemmatizer.fit_transform(reviews_lemmatizer)\n","print(type(reviews_lemmatizer_tfidf))\n","print(reviews_lemmatizer_tfidf.shape)\n","reviews_lemmatizer_tfidf = pd.DataFrame.sparse.from_spmatrix(reviews_lemmatizer_tfidf)\n","reviews_lemmatizer_tfidf = reviews_lemmatizer_tfidf.sparse.to_dense()\n","print(reviews_lemmatizer_tfidf.shape)"],"metadata":{"id":"UdM4NdSKoMzt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h3> Model Selection and Training </h3>\n","Mohamed Allam & Momen Mohamed: Naive Bayes\n","<br>\n","Mohamed Tarek & Mohamed Gaber: SVM\n","<br>\n","Yousef Ahmed & Hady Ahmed: Random Forest Classification\n","<br>\n","all will try the different datasets will be presented from feature engineering different algorithm"],"metadata":{"id":"qESW-zs0eA9P"}},{"cell_type":"code","source":["data.info()"],"metadata":{"id":"E1ICHYS5PQW6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data[\"Sentiment (Label)\"] = output"],"metadata":{"id":"jnIZSF3BI-IB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data.select_dtypes(int).corr()"],"metadata":{"id":"pm5RYJPHOVtw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["reviews_datasets = [reviews_lemmatizer_tfidf,reviews_stemmer_tfidf, reviews_lemmatizer_bag_of_words,\n","                    reviews_stemmer_bag_of_words, reviews_lemmatize_word_embedding,\n","                    reviews_stemmer_word_embedding, output]"],"metadata":{"id":"EXIX1sNNvW9b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["reviews_datasets_names = [\"reviews_lemmatizer_tfidf\",\"reviews_stemmer_tfidf\", \"reviews_lemmatizer_bag_of_words\",\n","                    \"reviews_stemmer_bag_of_words\", \"reviews_lemmatize_word_embedding\",\n","                    \"reviews_stemmer_word_embedding\", \"output\"]"],"metadata":{"id":"Qlf1TlBalBAe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h4> Naive Bayes Model </h4>"],"metadata":{"id":"rwKdMNZL66L-"}},{"cell_type":"code","source":["from sklearn.naive_bayes import MultinomialNB\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.model_selection import StratifiedShuffleSplit\n","\n","# Build a Gaussian Classifier\n","predictions = []\n","counter = -1\n","for i in reviews_datasets:\n","  model = MultinomialNB(alpha=0.2, fit_prior=True, force_alpha=True)\n","  counter = counter + 1\n","  if counter == 4:\n","    break\n","  X_train, X_test, y_train, y_test = train_test_split(i, reviews_datasets[-1], test_size=0.25, random_state=81)\n","#   sss = StratifiedShuffleSplit(n_splits=20, test_size=0.5, random_state=0)\n","#   sss.get_n_splits(i, reviews_datasets[-1])\n","\n","#   scores = []\n","#   s = []\n","# # using regression to get predicted data\n","#   for train_index, test_index in sss.split(i, reviews_datasets[-1]):\n","#      X_train, X_test = i.iloc[train_index, :], i.iloc[test_index, :]\n","#      y_train, y_test = reviews_datasets[-1].iloc[train_index, :], reviews_datasets[-1].iloc[test_index, :]\n","#      model.fit(X_train, y_train)\n","#      pred = model.predict(X_test)\n","#      scores.append(accuracy_score(y_test, pred))\n","#      pred = model.predict(X_train)\n","#      s.append(accuracy_score(y_train, pred))\n","\n","#get accuracy of each prediction\n","\n","# Model training\n","  model.fit(X_train, y_train)\n","\n","# Predict Output\n","  test_predict = model.predict(X_train)\n","  predicted = model.predict(X_test)\n","\n","  predictions.append(predicted)\n","  print(\"Dataset: \",reviews_datasets_names[counter])\n","  print(\"Train Accuracy: \", accuracy_score(y_train, test_predict))\n","  print(\"Test Accuracy: \", accuracy_score(y_test, predicted))\n","  print(\"Train Report: \", metrics.classification_report(y_train, test_predict))\n","  print(\"Test Report: \", metrics.classification_report(y_test, predicted))\n","\n","  metrics.recall_score\n","  metrics.precision_score"],"metadata":{"id":"e25LD0q5kHsb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.naive_bayes import MultinomialNB\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.model_selection import StratifiedShuffleSplit\n","\n","# Build a Gaussian Classifier\n","counter = -1\n","for i in reviews_datasets:\n","  counter = counter + 1\n","  model = GaussianNB()\n","  X_train, X_test, y_train, y_test = train_test_split(i, reviews_datasets[-1], test_size=0.25, random_state=61)\n","#   sss = StratifiedShuffleSplit(n_splits=20, test_size=0.5, random_state=0)\n","#   sss.get_n_splits(i, reviews_datasets[-1])\n","\n","#   scores = []\n","#   s = []\n","# # using regression to get predicted data\n","#   for train_index, test_index in sss.split(i, reviews_datasets[-1]):\n","#      X_train, X_test = i.iloc[train_index, :], i.iloc[test_index, :]\n","#      y_train, y_test = reviews_datasets[-1].iloc[train_index, :], reviews_datasets[-1].iloc[test_index, :]\n","#      model.fit(X_train, y_train)\n","#      pred = model.predict(X_test)\n","#      scores.append(accuracy_score(y_test, pred))\n","#      pred = model.predict(X_train)\n","#      s.append(accuracy_score(y_train, pred))\n","\n","#get accuracy of each prediction\n","\n","# Model training\n","  model.fit(X_train, y_train)\n","\n","# Predict Output\n","  test_predict = model.predict(X_train)\n","  predicted = model.predict(X_test)\n","\n","  predictions.append(predicted)\n","  print(\"Dataset: \",reviews_datasets_names[counter])\n","  print(\"Train Accuracy: \", accuracy_score(y_train, test_predict))\n","  print(\"Test Accuracy: \", accuracy_score(y_test, predicted))\n","  print(\"Train Report: \", metrics.classification_report(y_train, test_predict))\n","  print(\"Test Report: \", metrics.classification_report(y_test, predicted))\n","\n","  metrics.recall_score\n","  metrics.precision_score"],"metadata":{"id":"o70cD-9mzHh6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","<h4> SVM Model </h4>"],"metadata":{"id":"zCUGMJZj7Kay"}},{"cell_type":"code","source":["from sklearn.svm import SVC\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.model_selection import StratifiedShuffleSplit\n","\n","# Build a Gaussian Classifier\n","predictions = []\n","counter = -1\n","for i in reviews_datasets:\n","  model = SVC(kernel='sigmoid')\n","  counter = counter + 1\n","  X_train, X_test, y_train, y_test = train_test_split(i, reviews_datasets[-1], test_size=0.25, random_state=61)\n","#   sss = StratifiedShuffleSplit(n_splits=20, test_size=0.5, random_state=0)\n","#   sss.get_n_splits(i, reviews_datasets[-1])\n","\n","#   scores = []\n","#   s = []\n","# # using regression to get predicted data\n","#   for train_index, test_index in sss.split(i, reviews_datasets[-1]):\n","#      X_train, X_test = i.iloc[train_index, :], i.iloc[test_index, :]\n","#      y_train, y_test = reviews_datasets[-1].iloc[train_index, :], reviews_datasets[-1].iloc[test_index, :]\n","#      model.fit(X_train, y_train)\n","#      pred = model.predict(X_test)\n","#      scores.append(accuracy_score(y_test, pred))\n","#      pred = model.predict(X_train)\n","#      s.append(accuracy_score(y_train, pred))\n","\n","#get accuracy of each prediction\n","\n","# Model training\n","  model.fit(X_train, y_train)\n","\n","# Predict Output\n","  test_predict = model.predict(X_train)\n","  predicted = model.predict(X_test)\n","\n","  predictions.append(predicted)\n","  print(\"Dataset: \",reviews_datasets_names[counter])\n","  print(\"Train Accuracy: \", accuracy_score(y_train, test_predict))\n","  print(\"Test Accuracy: \", accuracy_score(y_test, predicted))\n","  print(\"Train Report: \", metrics.classification_report(y_train, test_predict))\n","  print(\"Test Report: \", metrics.classification_report(y_test, predicted))\n","\n","  metrics.recall_score\n","  metrics.precision_score"],"metadata":{"id":"0-bgxEuikibD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.svm import SVC\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.model_selection import StratifiedShuffleSplit\n","\n","# Build a Gaussian Classifier\n","predictions = []\n","counter = -1\n","for i in reviews_datasets:\n","  model = SVC(kernel='rbf')\n","  counter = counter + 1\n","  X_train, X_test, y_train, y_test = train_test_split(i, reviews_datasets[-1], test_size=0.25, random_state=61)\n","#   sss = StratifiedShuffleSplit(n_splits=20, test_size=0.5, random_state=0)\n","#   sss.get_n_splits(i, reviews_datasets[-1])\n","\n","#   scores = []\n","#   s = []\n","# # using regression to get predicted data\n","#   for train_index, test_index in sss.split(i, reviews_datasets[-1]):\n","#      X_train, X_test = i.iloc[train_index, :], i.iloc[test_index, :]\n","#      y_train, y_test = reviews_datasets[-1].iloc[train_index, :], reviews_datasets[-1].iloc[test_index, :]\n","#      model.fit(X_train, y_train)\n","#      pred = model.predict(X_test)\n","#      scores.append(accuracy_score(y_test, pred))\n","#      pred = model.predict(X_train)\n","#      s.append(accuracy_score(y_train, pred))\n","\n","#get accuracy of each prediction\n","\n","# Model training\n","  model.fit(X_train, y_train)\n","\n","# Predict Output\n","  test_predict = model.predict(X_train)\n","  predicted = model.predict(X_test)\n","\n","  predictions.append(predicted)\n","  print(\"Dataset: \",reviews_datasets_names[counter])\n","  print(\"Train Accuracy: \", accuracy_score(y_train, test_predict))\n","  print(\"Test Accuracy: \", accuracy_score(y_test, predicted))\n","  print(\"Train Report: \", metrics.classification_report(y_train, test_predict))\n","  print(\"Test Report: \", metrics.classification_report(y_test, predicted))\n","\n","  metrics.recall_score\n","  metrics.precision_score"],"metadata":{"id":"kt7LsY8cVCNF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h4> Random Forest Model </h4>"],"metadata":{"id":"uAOxXPJw7hWE"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.svm import SVC\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.model_selection import StratifiedShuffleSplit\n","\n","# Build a Gaussian Classifier\n","predictions = []\n","counter = -1\n","for i in reviews_datasets:\n","  model = RandomForestClassifier()\n","  counter = counter + 1\n","  X_train, X_test, y_train, y_test = train_test_split(i, reviews_datasets[-1], test_size=0.25, random_state=61)\n","#   sss = StratifiedShuffleSplit(n_splits=20, test_size=0.5, random_state=0)\n","#   sss.get_n_splits(i, reviews_datasets[-1])\n","\n","#   scores = []\n","#   s = []\n","# # using regression to get predicted data\n","#   for train_index, test_index in sss.split(i, reviews_datasets[-1]):\n","#      X_train, X_test = i.iloc[train_index, :], i.iloc[test_index, :]\n","#      y_train, y_test = reviews_datasets[-1].iloc[train_index, :], reviews_datasets[-1].iloc[test_index, :]\n","#      model.fit(X_train, y_train)\n","#      pred = model.predict(X_test)\n","#      scores.append(accuracy_score(y_test, pred))\n","#      pred = model.predict(X_train)\n","#      s.append(accuracy_score(y_train, pred))\n","\n","#get accuracy of each prediction\n","\n","# Model training\n","  model.fit(X_train, y_train)\n","\n","# Predict Output\n","  test_predict = model.predict(X_train)\n","  predicted = model.predict(X_test)\n","\n","  predictions.append(predicted)\n","  print(\"Dataset: \",reviews_datasets_names[counter])\n","  print(\"Train Accuracy: \", accuracy_score(y_train, test_predict))\n","  print(\"Test Accuracy: \", accuracy_score(y_test, predicted))\n","  print(\"Train Report: \", metrics.classification_report(y_train, test_predict))\n","  print(\"Test Report: \", metrics.classification_report(y_test, predicted))\n","\n","  metrics.recall_score\n","  metrics.precision_score"],"metadata":{"id":"aCxlTxjoj0Wr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.svm import SVC\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.model_selection import StratifiedShuffleSplit\n","\n","# Build a Gaussian Classifier\n","predictions = []\n","counter = -1\n","for i in reviews_datasets:\n","  model = RandomForestClassifier(criterion=\"entropy\")\n","  counter = counter + 1\n","  X_train, X_test, y_train, y_test = train_test_split(i, reviews_datasets[-1], test_size=0.25, random_state=61)\n","#   sss = StratifiedShuffleSplit(n_splits=20, test_size=0.5, random_state=0)\n","#   sss.get_n_splits(i, reviews_datasets[-1])\n","\n","#   scores = []\n","#   s = []\n","# # using regression to get predicted data\n","#   for train_index, test_index in sss.split(i, reviews_datasets[-1]):\n","#      X_train, X_test = i.iloc[train_index, :], i.iloc[test_index, :]\n","#      y_train, y_test = reviews_datasets[-1].iloc[train_index, :], reviews_datasets[-1].iloc[test_index, :]\n","#      model.fit(X_train, y_train)\n","#      pred = model.predict(X_test)\n","#      scores.append(accuracy_score(y_test, pred))\n","#      pred = model.predict(X_train)\n","#      s.append(accuracy_score(y_train, pred))\n","\n","#get accuracy of each prediction\n","\n","# Model training\n","  model.fit(X_train, y_train)\n","\n","# Predict Output\n","  test_predict = model.predict(X_train)\n","  predicted = model.predict(X_test)\n","\n","  predictions.append(predicted)\n","  print(\"Dataset: \",reviews_datasets_names[counter])\n","  print(\"Train Accuracy: \", accuracy_score(y_train, test_predict))\n","  print(\"Test Accuracy: \", accuracy_score(y_test, predicted))\n","  print(\"Train Report: \", metrics.classification_report(y_train, test_predict))\n","  print(\"Test Report: \", metrics.classification_report(y_test, predicted))\n","\n","  metrics.recall_score\n","  metrics.precision_score"],"metadata":{"id":"QH5Jml8Bm0XW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h4> Multinomial Logistic Regression </h4>"],"metadata":{"id":"6t7HnSYyiUNT"}},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import StratifiedShuffleSplit\n","\n","# Build a Gaussian Classifier\n","predictions = []\n","counter = -1\n","for i in reviews_datasets:\n","  model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n","  counter = counter + 1\n","  X_train, X_test, y_train, y_test = train_test_split(i, reviews_datasets[-1], test_size=0.25, random_state=61)\n","#   sss = StratifiedShuffleSplit(n_splits=20, test_size=0.5, random_state=0)\n","#   sss.get_n_splits(i, reviews_datasets[-1])\n","\n","#   scores = []\n","#   s = []\n","# # using regression to get predicted data\n","#   for train_index, test_index in sss.split(i, reviews_datasets[-1]):\n","#      X_train, X_test = i.iloc[train_index, :], i.iloc[test_index, :]\n","#      y_train, y_test = reviews_datasets[-1].iloc[train_index, :], reviews_datasets[-1].iloc[test_index, :]\n","#      model.fit(X_train, y_train)\n","#      pred = model.predict(X_test)\n","#      scores.append(accuracy_score(y_test, pred))\n","#      pred = model.predict(X_train)\n","#      s.append(accuracy_score(y_train, pred))\n","\n","#get accuracy of each prediction\n","\n","# Model training\n","  model.fit(X_train, y_train)\n","\n","# Predict Output\n","  test_predict = model.predict(X_train)\n","  predicted = model.predict(X_test)\n","\n","  predictions.append(predicted)\n","  print(\"Dataset: \",reviews_datasets_names[counter])\n","  print(\"Train Accuracy: \", accuracy_score(y_train, test_predict))\n","  print(\"Test Accuracy: \", accuracy_score(y_test, predicted))\n","  print(\"Train Report: \", metrics.classification_report(y_train, test_predict))\n","  print(\"Test Report: \", metrics.classification_report(y_test, predicted))\n","\n","  metrics.recall_score\n","  metrics.precision_score"],"metadata":{"id":"QOLItiP1h2kf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h2> Model Evaluation </h2>"],"metadata":{"id":"CJMritAieKDy"}},{"cell_type":"code","source":["from sklearn.naive_bayes import MultinomialNB\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.model_selection import StratifiedShuffleSplit\n","\n","# Build a Gaussian Classifier\n","predictions = []\n","i = reviews_datasets[2]\n","model1 = GaussianNB()\n","counter = counter + 1\n","X_train, X_test, y_train, y_test = train_test_split(i, reviews_datasets[-1], test_size=0.25, random_state=61)\n","#   sss = StratifiedShuffleSplit(n_splits=20, test_size=0.5, random_state=0)\n","#   sss.get_n_splits(i, reviews_datasets[-1])\n","\n","#   scores = []\n","#   s = []\n","# # using regression to get predicted data\n","#   for train_index, test_index in sss.split(i, reviews_datasets[-1]):\n","#      X_train, X_test = i.iloc[train_index, :], i.iloc[test_index, :]\n","#      y_train, y_test = reviews_datasets[-1].iloc[train_index, :], reviews_datasets[-1].iloc[test_index, :]\n","#      model.fit(X_train, y_train)\n","#      pred = model.predict(X_test)\n","#      scores.append(accuracy_score(y_test, pred))\n","#      pred = model.predict(X_train)\n","#      s.append(accuracy_score(y_train, pred))\n","\n","#get accuracy of each prediction\n","\n","# Model training\n","model1.fit(X_train, y_train)\n","\n","# Predict Output\n","test_predict = model1.predict(X_train)\n","predicted = model1.predict(X_test)\n","\n","predictions.append(predicted)\n","print(\"Dataset: \",reviews_datasets_names[counter])\n","print(\"Train Accuracy: \", accuracy_score(y_train, test_predict))\n","print(\"Test Accuracy: \", accuracy_score(y_test, predicted))\n","print(\"Train Report: \", metrics.classification_report(y_train, test_predict))\n","print(\"Test Report: \", metrics.classification_report(y_test, predicted))\n","\n","metrics.recall_score\n","metrics.precision_score"],"metadata":{"id":"7U6Ox1EFwrcE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.naive_bayes import MultinomialNB\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.model_selection import StratifiedShuffleSplit\n","\n","# Build a Gaussian Classifier\n","for i in range(0, 100):\n","  predictions = []\n","  model2 = MultinomialNB(alpha=0.2, fit_prior=True, force_alpha=True)\n","  X_train, X_test, y_train, y_test = train_test_split(reviews_datasets[0], reviews_datasets[-1], test_size=0.25, random_state=i)\n","#   sss = StratifiedShuffleSplit(n_splits=20, test_size=0.5, random_state=0)\n","#   sss.get_n_splits(i, reviews_datasets[-1])\n","\n","#   scores = []\n","#   s = []\n","# # using regression to get predicted data\n","#   for train_index, test_index in sss.split(i, reviews_datasets[-1]):\n","#      X_train, X_test = i.iloc[train_index, :], i.iloc[test_index, :]\n","#      y_train, y_test = reviews_datasets[-1].iloc[train_index, :], reviews_datasets[-1].iloc[test_index, :]\n","#      model.fit(X_train, y_train)\n","#      pred = model.predict(X_test)\n","#      scores.append(accuracy_score(y_test, pred))\n","#      pred = model.predict(X_train)\n","#      s.append(accuracy_score(y_train, pred))\n","\n","#get accuracy of each prediction\n","\n","# Model training\n","  model2.fit(X_train, y_train)\n","\n","# Predict Output\n","  test_predict = model2.predict(X_train)\n","  predicted = model2.predict(X_test)\n","\n","  predictions.append(predicted)\n","\n","  print(\"Dataset: \",reviews_datasets_names[counter])\n","  print(i)\n","  print(\"Train Accuracy: \", accuracy_score(y_train, test_predict))\n","  print(\"Test Accuracy: \", accuracy_score(y_test, predicted))\n","  print(\"Train Report: \", metrics.classification_report(y_train, test_predict))\n","  print(\"Test Report: \", metrics.classification_report(y_test, predicted))\n","\n","metrics.recall_score\n","metrics.precision_score"],"metadata":{"id":"bhGHkRL_t_P9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.naive_bayes import MultinomialNB\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.model_selection import StratifiedShuffleSplit\n","\n","# Build a Gaussian Classifier\n","predictions = []\n","model2 = MultinomialNB(alpha=0.2, fit_prior=True, force_alpha=True)\n","X_train, X_test, y_train, y_test = train_test_split(reviews_datasets[0], reviews_datasets[-1], test_size=0.25, random_state=81)\n","#   sss = StratifiedShuffleSplit(n_splits=20, test_size=0.5, random_state=0)\n","#   sss.get_n_splits(i, reviews_datasets[-1])\n","\n","#   scores = []\n","#   s = []\n","# # using regression to get predicted data\n","#   for train_index, test_index in sss.split(i, reviews_datasets[-1]):\n","#      X_train, X_test = i.iloc[train_index, :], i.iloc[test_index, :]\n","#      y_train, y_test = reviews_datasets[-1].iloc[train_index, :], reviews_datasets[-1].iloc[test_index, :]\n","#      model.fit(X_train, y_train)\n","#      pred = model.predict(X_test)\n","#      scores.append(accuracy_score(y_test, pred))\n","#      pred = model.predict(X_train)\n","#      s.append(accuracy_score(y_train, pred))\n","\n","#get accuracy of each prediction\n","\n","# Model training\n","model2.fit(X_train, y_train)\n","\n","# Predict Output\n","test_predict = model2.predict(X_train)\n","predicted = model2.predict(X_test)\n","\n","predictions.append(predicted)\n","\n","print(reviews_datasets_names[0])\n","print(accuracy_score(y_train, test_predict))\n","print(accuracy_score(y_test, predicted))\n","print(metrics.classification_report(y_train, test_predict))\n","print(metrics.classification_report(y_test, predicted))\n","\n","metrics.recall_score\n","metrics.precision_score"],"metadata":{"id":"kDLJ3-_VhwYR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from fastapi import FastAPI\n","app = FastAPI()"],"metadata":{"id":"vdfZCVkDxkWi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["joblib.dump(tfidf_vectorizer_lemmatizer, \"tfidf.pkl\")"],"metadata":{"id":"-D3R_4AfsCKv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["joblib.dump(bow_vectorizer_lemmatizer, \"bag_of_words.pkl\")"],"metadata":{"id":"V6SEcZ4vQX8y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocessing2(text):\n","  text = text.strip()\n","  text = text.lower()\n","  text = remove_hashtags(text)\n","  text = remove_punctuation(text)\n","  text = remove_stop_words(text)\n","  text = remove_dash(text)\n","  text = replace_emojies(text)\n","  text = change_to_orgin_lemmatizer(text)\n","  print(text)\n","  text = tfidf_vectorizer_lemmatizer.transform(list([text, \"hell\"]))\n","  text = text.toarray()\n","  return text[0].reshape(1, -1)"],"metadata":{"id":"7M0ZoWxozEls"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocessing1(text):\n","  text = text.strip()\n","  text = text.lower()\n","  text = remove_hashtags(text)\n","  text = remove_punctuation(text)\n","  text = remove_stop_words(text)\n","  text = remove_dash(text)\n","  text = replace_emojies(text)\n","  text = change_to_orgin_lemmatizer(text)\n","  print(text)\n","  text = bow_vectorizer_lemmatizer.transform(list([text, \"hell\"]))\n","  text = text.toarray()\n","  return text[0].reshape(1, -1)"],"metadata":{"id":"8u5DpJ6yuSah"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["processing = FunctionTransformer(preprocessing1)\n","sk_pipe = Pipeline([(\"trans\", processing), (\"model\", model1)])"],"metadata":{"id":"7lg3QKdfEiq9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["processing = FunctionTransformer(preprocessing2)\n","sk_pipe2 = Pipeline([(\"trans\", processing), (\"model\", model2)])"],"metadata":{"id":"2dOnuUp8uipT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = \"I hate being sad\"\n","print(sk_pipe.predict(text))\n","print(sk_pipe2.predict(text))"],"metadata":{"id":"IPRf4zIwupsq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["joblib.dump(sk_pipe2, \"sentiment_analysis_model2.pkl\")"],"metadata":{"id":"078duIDJGG27"},"execution_count":null,"outputs":[]}]}